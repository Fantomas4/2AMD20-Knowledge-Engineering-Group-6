{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> States included in CBP but not in Bachelor Majors dataset: 7\n",
      "0            Maine\n",
      "1      Puerto Rico\n",
      "2           Alaska\n",
      "3          Wyoming\n",
      "4          Montana\n",
      "5     South Dakota\n",
      "6    West Virginia\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bf259f04854613aad66fad8dbc10df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=13.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93dccdbf8814dbe988f87103683704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report cbp_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n",
      "Report bachelor_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n",
      "> Saving preprocessed datasets to .csv files...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "\n",
    "# Load the dataset (example: iris dataset)\n",
    "df_1 = pd.read_csv('datasets/CBP2019.CB1900CBP-2023-05-14T012245.csv')\n",
    "df_2 = pd.read_csv('datasets/Bachelor_Degree_Majors.csv')\n",
    "df_3 = pd.read_csv('datasets/state_regions.csv')\n",
    "\n",
    "\n",
    "# Drop any columns from df_1 that we do not need for our analysis\n",
    "columns_to_drop = [\"Year (YEAR)\", \"Meaning of NAICS code (NAICS2017_LABEL)\", \"2017 NAICS code (NAICS2017)\",\n",
    "                   \"Meaning of Legal form of organization code (LFO_LABEL)\"]\n",
    "df_1 = df_1.drop(columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "# Since we dropped some columns, some rows have become duplicates of others. Thus, we proceed to drop them.\n",
    "df_1 = df_1.drop_duplicates()\n",
    "\n",
    "\n",
    "# Rename the columns of df_1 to make them easier to work with\n",
    "column_rename_mapping = {\n",
    "    \"Geographic Area Name (NAME)\": \"State\",\n",
    "    \"Meaning of Employment size of establishments code (EMPSZES_LABEL)\": \"Business size\",\n",
    "    \"Number of establishments (ESTAB)\": \"#Establishments\",\n",
    "    \"Annual payroll ($1,000) (PAYANN)\": \"Average annual payroll\",\n",
    "    \"First-quarter payroll ($1,000) (PAYQTR1)\": \"Average first-quarter payroll\",\n",
    "    \"Number of employees (EMP)\": \"Average #employees\"\n",
    "}\n",
    "df_1.rename(columns=column_rename_mapping, inplace=True)\n",
    "\n",
    "\n",
    "# Contradiction mitigation: For the CPB dataset (df_1), drop any rows where \"Business size\" == \"All establishments\"\n",
    "df_1 = df_1[(df_1[\"Business size\"] != \"All establishments\")]\n",
    "\n",
    "# Contradiction mitigatino: For the Bachelor's dataset, replace all \"25 and older\" values of the \"Age Group\" column\n",
    "# with the value \"younger than 25\"\n",
    "df_2['Age Group'] = df_2['Age Group'].replace('25 and older', 'younger than 25')\n",
    "\n",
    "\n",
    "# For the CPB dataset (df_1), only keep the rows where the value of the \"Business size\" attribute refers to a company\n",
    "# that represents a \"major\" competitor, according to our client's criteria\n",
    "values_to_keep = [\n",
    "    \"Establishments with 50 to 99 employees\",\n",
    "    \"Establishments with 100 to 249 employees\",\n",
    "    \"Establishments with 250 to 499 employees\",\n",
    "    \"Establishments with 500 to 999 employees\",\n",
    "    \"Establishments with 1,000 employees or more\"\n",
    "]\n",
    "df_1 = df_1[df_1[\"Business size\"].isin(values_to_keep)]\n",
    "\n",
    "\n",
    "# Get the non-common values between the \"State\" columns of the 2 datasets (df_1 and df_2)\n",
    "symmetric_difference = pd.Series(list(set(df_1['State']).symmetric_difference(set(df_2['State']))))\n",
    "print(\"\\n> States included in CBP but not in Bachelor Majors dataset: {}\".format(len(symmetric_difference)))\n",
    "print(symmetric_difference)\n",
    "\n",
    "\n",
    "# We will to drop the rows in df_1 and df_2 that contain any of the non-common (State) values.\n",
    "df_1 = df_1[~df_1['State'].isin(symmetric_difference.tolist())]\n",
    "df_2 = df_2[~df_2['State'].isin(symmetric_difference.tolist())]\n",
    "\n",
    "\n",
    "# Remove \",\" from all numeric values in the CPB dataframe\n",
    "# Loop through each column in the DataFrame\n",
    "for column in df_1.columns:\n",
    "    # Remove commas from values\n",
    "    df_1[column] = df_1[column].str.replace(',', '')\n",
    "\n",
    "# Remove \",\" from all numeric values in the Bachelor's dataframe\n",
    "# Loop through each column in the DataFrame\n",
    "for column in df_2.columns:\n",
    "    # Remove commas from values\n",
    "    df_2[column] = df_2[column].str.replace(',', '')\n",
    "\n",
    "# Convert df_1 number columns to numeric values\n",
    "numeric_columns = [\"#Establishments\", \"Average annual payroll\", \"Average first-quarter payroll\",\n",
    "                   \"Average #employees\"]\n",
    "df_1[numeric_columns] = df_1[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "# Convert df_2 number columns to numeric values\n",
    "numeric_columns = [\"Bachelor's Degree Holders\", \"Science and Engineering\", \"Science and Engineering Related Fields\",\n",
    "                   \"Business\", \"Education\", \"Arts, Humanities and Others\"]\n",
    "df_2[numeric_columns] = df_2[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "# Add the information from the \"State Regions\" dataset (df_3) to CBP (df_1) as a new column\n",
    "df_1 = pd.merge(df_1, df_3, on='State', how='left')\n",
    "\n",
    "# ====== Generate a new \"Men to women degree holders ratio\" column in df_1 that contains the men to women\n",
    "# bachelor holders ratio for each state\n",
    "# Filter the DataFrame for \"Male\" and \"Female\" separately\n",
    "male_df = df_2[df_2['Sex'] == 'Male']\n",
    "female_df = df_2[df_2['Sex'] == 'Female']\n",
    "\n",
    "# Group by \"State\" and calculate the sum of \"Bachelor's Degree Holders\" for each gender\n",
    "male_counts = male_df.groupby('State')['Bachelor\\'s Degree Holders'].sum()\n",
    "female_counts = female_df.groupby('State')['Bachelor\\'s Degree Holders'].sum()\n",
    "\n",
    "# Calculate the ratio of men to women degree holdersfor each state\n",
    "ratio = male_counts / female_counts\n",
    "\n",
    "# Create a new column in df_1 and map the men/women ratios there based on the \"State\" value of each entry.\n",
    "df_1['Men to women degree holders ratio'] = df_1['State'].map(ratio)\n",
    "\n",
    "\n",
    "# == Determine the field that has the largest and second-largest number of graduates per State\n",
    "# Filter the dataset to keep only rows where \"Sex\" is equal to \"Total\"\n",
    "filtered_df = df_2[df_2[\"Sex\"] == \"Total\"]\n",
    "\n",
    "# Group the filtered DataFrame by \"State\"\n",
    "grouped_df = filtered_df.groupby(\"State\")\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "state_column_dict = {}\n",
    "\n",
    "# == Determine the most popular field of studies\n",
    "# Iterate over each distinct value of \"State\"\n",
    "for state, group in grouped_df:\n",
    "    # Calculate the summed values for each column\n",
    "    summed_values = group[[\"Science and Engineering\", \"Science and Engineering Related Fields\", \"Business\", \"Education\",\n",
    "                           \"Arts, Humanities and Others\"]].sum()\n",
    "\n",
    "    # Find the column with the highest summed value\n",
    "    max_column = summed_values.idxmax()\n",
    "\n",
    "    # Save the column name to the state:column dictionary\n",
    "    state_column_dict[state] = max_column\n",
    "\n",
    "# Add the new column to the df_2 dataframe\n",
    "df_1[\"Most popular degree field\"] = df_1[\"State\"].map(state_column_dict)\n",
    "\n",
    "# ==== Determine the 2nd most popular field of studies\n",
    "# Iterate over each distinct value of \"State\"\n",
    "for state, group in grouped_df:\n",
    "    # Calculate the summed values for each column\n",
    "    summed_values = group[[\"Science and Engineering\", \"Science and Engineering Related Fields\", \"Business\", \"Education\",\n",
    "                           \"Arts, Humanities and Others\"]].sum()\n",
    "\n",
    "    # Sort the summed values in descending order and get the column name with the second largest summed value\n",
    "    second_largest_column = summed_values.sort_values(ascending=False).index[1]\n",
    "\n",
    "    # Save the column name to the state:column dictionary\n",
    "    state_column_dict[state] = second_largest_column\n",
    "\n",
    "# Add the new column to the df_2 dataframe\n",
    "df_1[\"2nd Most popular degree field\"] = df_1[\"State\"].map(state_column_dict)\n",
    "\n",
    "\n",
    "# ====== Add a new \"#(Mid)Senior degree holders\" column to df_1 that is generated by summing the values of\n",
    "# \"Bachelor's Degree Holders\" for the \"25-39\" and \"40-64\" age groups of the \"Bachelor's\" dataset (df_2) for every State.\n",
    "# NOTE: These age groups are considered to include both sexes (\"Total\" value of the \"Sex\" attribute).\n",
    "# Filter the dataset to keep only rows where \"Sex\" is equal to \"Total\"\n",
    "filtered_df = df_2[df_2[\"Sex\"] == \"Total\"]\n",
    "\n",
    "# Group the filtered DataFrame by \"State\"\n",
    "grouped_df = filtered_df.groupby(\"State\")\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "state_sum_dict = {}\n",
    "\n",
    "# Iterate over each distinct value of \"State\"\n",
    "for state, group in grouped_df:\n",
    "    # Filter the group based on the specified conditions and calculate the sum of \"Bachelor's Degree Holders\"\n",
    "    age_group_condition = (group[\"Age Group\"].isin([\"25 to 39\", \"40 to 64\"]))\n",
    "    sum_value = group.loc[age_group_condition, \"Bachelor's Degree Holders\"].sum()\n",
    "\n",
    "    # Save the sum to the state:sum dictionary\n",
    "    state_sum_dict[state] = sum_value\n",
    "\n",
    "df_1[\"#(Mid)Senior degree holders\"] = df_1[\"State\"].map(state_sum_dict)\n",
    "\n",
    "\n",
    "# ====== Generate a new \"Degree holders to establishments ratio\" column that holds the\n",
    "# #Degree Holders/#Business establishments ratio per State, taking into consideration both sexes.\n",
    "# Group the df_2 DataFrame by \"State\" and filter rows where \"Sex\" is \"Total\"\n",
    "filtered_df = df_2[df_2['Sex'] == 'Total'].groupby('State')\n",
    "\n",
    "# Sum the values of \"Age Group\" per state\n",
    "degree_holders_per_state = filtered_df['Bachelor\\'s Degree Holders'].sum()\n",
    "\n",
    "# Group the df_1 DataFrame by \"State\"\n",
    "filtered_df = df_1.groupby(\"State\")\n",
    "\n",
    "# Sum the values of \"Business size\" per State\n",
    "establishments_per_state = filtered_df[\"#Establishments\"].sum()\n",
    "\n",
    "# Calculate the ratio\n",
    "ratio = degree_holders_per_state / establishments_per_state\n",
    "\n",
    "# Create a new column in df_1 and map the #Degree Holders/#Business establishments ratios there,\n",
    "# based on the \"State\" value of each entry.\n",
    "df_1['Degree holders to establishments ratio'] = df_1['State'].map(ratio)\n",
    "\n",
    "\n",
    "# For the Bachelor's dataset (df_2), drop any rows where \"Sex\" == \"Total\"\n",
    "df_2 = df_2[(df_2[\"Sex\"] != \"Total\")]\n",
    "\n",
    "\n",
    "# Generate the analysis report\n",
    "report_1 = sv.analyze(df_1)\n",
    "report_2 = sv.analyze(df_2)\n",
    "\n",
    "# Display the report in the browser\n",
    "report_1.show_html('cbp_report.html')\n",
    "report_2.show_html('bachelor_report.html')\n",
    "\n",
    "print(\"> Saving preprocessed datasets to .csv files...\")\n",
    "df_1.to_csv('datasets/CBP_preprocessed.csv', index=False)\n",
    "df_2.to_csv('datasets/Bachelor_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities = pd.read_csv('datasets/National Universities Rankings.csv')\n",
    "business = pd.read_csv('datasets/BDSTIMESERIES.BDSGEO-2023-05-31T192640.csv')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A firm is a business organization or entity consisting of one or more domestic establishments (locations) under common ownership or control. Dataset has firms and establishments, we look at establishments (also, dataset has only info about exited firms, not born)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate of establishments born during the last 12 months = The number of establishments born during last 12 months divided by the average number of estabs in year t (current year) and year t-1 (prior year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate of establishments exited during the last 12 months = The number of establishments that exited during last 12 months divided by the average number of estabs in year t (current year) and year t-1 (prior year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns of business to make them easier to work with\n",
    "column_rename_mapping = {\n",
    "    \"Geographic Area Name (NAME)\": \"State\",\n",
    "    \"Year (YEAR)\": \"Year\",\n",
    "    \"Rate of establishments born during the last 12 months (ESTABS_ENTRY_RATE)\": \"Rate establishments born\",\n",
    "    \"Rate of establishments exited during the last 12 months (ESTABS_EXIT_RATE)\": \"Rate establishments exited\",\n",
    "}\n",
    "business.rename(columns=column_rename_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = business[['State', 'Year', \"Rate establishments born\", \"Rate establishments exited\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data for the last decade\n",
    "business_recent = business[(business['Year'] >= 2009) & (business['Year'] <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rates for the last decade\n",
    "business_agg = business_recent.groupby('State')[['Rate establishments born', 'Rate establishments exited']].mean()\n",
    "business_agg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between born and exited rate (kind of \"clean\" born rate)\n",
    "# If negative: more exited than born\n",
    "business_agg['Rate born - exited'] = business_agg['Rate establishments born'] - business_agg['Rate establishments exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 51)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_1['State'].unique().tolist()), len(business_agg['State'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> States included in Business Dynamics but not in CBP dataset: 6\n",
      "0            Maine\n",
      "1           Alaska\n",
      "2          Wyoming\n",
      "3          Montana\n",
      "4     South Dakota\n",
      "5    West Virginia\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get the non-common values between the \"State\" columns of the 2 datasets (df_1 and business)\n",
    "symmetric_difference = pd.Series(list(set(business_agg['State']).symmetric_difference(set(df_1['State']))))\n",
    "print(\"\\n> States included in Business Dynamics but not in CBP dataset: {}\".format(len(symmetric_difference)))\n",
    "print(symmetric_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will to drop the rows in business_agg that contain any of the non-common (State) values.\n",
    "business_agg = business_agg[~business_agg['State'].isin(symmetric_difference.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset with states abbreviatios to work with universities ranking\n",
    "states = pd.read_csv('datasets/state_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities['State Abbr'] = universities['Location'].str[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(universities, states, left_on='State Abbr', right_on = 'Alpha code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add full state name\n",
    "universities['State'] = merged['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select necessary columns\n",
    "universities = universities[['Name', 'Rank', 'State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> States included in Universities but not in CBP dataset: 6\n",
      "0            Maine\n",
      "1           Alaska\n",
      "2          Wyoming\n",
      "3          Montana\n",
      "4     South Dakota\n",
      "5    West Virginia\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "symmetric_difference = pd.Series(list(set(universities['State']).symmetric_difference(set(df_1['State']))))\n",
    "print(\"\\n> States included in Universities but not in CBP dataset: {}\".format(len(symmetric_difference)))\n",
    "print(symmetric_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will to drop the rows in universities that contain any of the non-common (State) values.\n",
    "universities = universities[~universities['State'].isin(symmetric_difference.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average, max and min rating of universities in a state: the LOWER the better (because rating starts at 1 = best university)\n",
    "universities_agg = universities.groupby('State')[['Rank']].agg({'mean', 'max', 'min'})\n",
    "universities_agg.reset_index(inplace = True)\n",
    "universities_agg.columns = universities_agg.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities_agg.rename(columns={\"mean\": \"Average rank\", 'max': 'Max rank', 'min': 'Min rank', '': 'State'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = universities_agg.sort_values('Min rank')\n",
    "# Top 10 states based on ranking\n",
    "best_states = sorted_df.head(10)['State'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities_agg['State with top universities'] = universities_agg.apply(lambda x: 'Yes' if x['State'] in best_states else 'No', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join business dynamics and universities datasets\n",
    "final_extra = pd.merge(business_agg, universities_agg, on = 'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Rate establishments born</th>\n",
       "      <th>Rate establishments exited</th>\n",
       "      <th>Rate born - exited</th>\n",
       "      <th>Min rank</th>\n",
       "      <th>Max rank</th>\n",
       "      <th>Average rank</th>\n",
       "      <th>State with top universities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>8.094364</td>\n",
       "      <td>8.360364</td>\n",
       "      <td>-0.266000</td>\n",
       "      <td>197</td>\n",
       "      <td>202</td>\n",
       "      <td>199.500000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>10.948182</td>\n",
       "      <td>10.689182</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>202</td>\n",
       "      <td>210</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>8.583182</td>\n",
       "      <td>8.523091</td>\n",
       "      <td>0.060091</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>10.854000</td>\n",
       "      <td>10.185727</td>\n",
       "      <td>0.668273</td>\n",
       "      <td>56</td>\n",
       "      <td>79</td>\n",
       "      <td>66.909091</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>11.497455</td>\n",
       "      <td>10.652545</td>\n",
       "      <td>0.844909</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>8.314545</td>\n",
       "      <td>8.707455</td>\n",
       "      <td>-0.392909</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>9.752455</td>\n",
       "      <td>9.424727</td>\n",
       "      <td>0.327727</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>9.878000</td>\n",
       "      <td>8.755091</td>\n",
       "      <td>1.122909</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>12.417545</td>\n",
       "      <td>11.378727</td>\n",
       "      <td>1.038818</td>\n",
       "      <td>164</td>\n",
       "      <td>171</td>\n",
       "      <td>169.428571</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>10.499727</td>\n",
       "      <td>10.142000</td>\n",
       "      <td>0.357727</td>\n",
       "      <td>133</td>\n",
       "      <td>135</td>\n",
       "      <td>134.500000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>8.315182</td>\n",
       "      <td>8.340455</td>\n",
       "      <td>-0.025273</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>10.589909</td>\n",
       "      <td>9.993909</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>9.197909</td>\n",
       "      <td>9.300545</td>\n",
       "      <td>-0.102636</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>27.363636</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>8.036636</td>\n",
       "      <td>8.122545</td>\n",
       "      <td>-0.085909</td>\n",
       "      <td>111</td>\n",
       "      <td>118</td>\n",
       "      <td>116.600000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>7.462727</td>\n",
       "      <td>7.417000</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>8.135545</td>\n",
       "      <td>8.350273</td>\n",
       "      <td>-0.214727</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>8.276636</td>\n",
       "      <td>8.314818</td>\n",
       "      <td>-0.038182</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>8.481545</td>\n",
       "      <td>8.331727</td>\n",
       "      <td>0.149818</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>9.151000</td>\n",
       "      <td>9.095273</td>\n",
       "      <td>0.055727</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>8.739909</td>\n",
       "      <td>8.409545</td>\n",
       "      <td>0.330364</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>14.266667</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>8.451545</td>\n",
       "      <td>8.726364</td>\n",
       "      <td>-0.274818</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>8.604455</td>\n",
       "      <td>8.468364</td>\n",
       "      <td>0.136091</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>7.921000</td>\n",
       "      <td>8.195000</td>\n",
       "      <td>-0.274000</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>9.956273</td>\n",
       "      <td>9.533545</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>124</td>\n",
       "      <td>133</td>\n",
       "      <td>128.142857</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>8.109364</td>\n",
       "      <td>7.676727</td>\n",
       "      <td>0.432636</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>12.341364</td>\n",
       "      <td>11.432364</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>8.321455</td>\n",
       "      <td>8.381000</td>\n",
       "      <td>-0.059545</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>9.776727</td>\n",
       "      <td>9.990909</td>\n",
       "      <td>-0.214182</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>8.791182</td>\n",
       "      <td>9.232091</td>\n",
       "      <td>-0.440909</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New York</td>\n",
       "      <td>10.345636</td>\n",
       "      <td>9.862545</td>\n",
       "      <td>0.483091</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "      <td>45.380952</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>9.536364</td>\n",
       "      <td>9.160818</td>\n",
       "      <td>0.375545</td>\n",
       "      <td>79</td>\n",
       "      <td>86</td>\n",
       "      <td>82.250000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>9.134545</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>1.224545</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>7.756455</td>\n",
       "      <td>8.197818</td>\n",
       "      <td>-0.441364</td>\n",
       "      <td>152</td>\n",
       "      <td>164</td>\n",
       "      <td>157.222222</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>8.944455</td>\n",
       "      <td>8.746364</td>\n",
       "      <td>0.198091</td>\n",
       "      <td>194</td>\n",
       "      <td>197</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>9.888273</td>\n",
       "      <td>9.348818</td>\n",
       "      <td>0.539455</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>7.975364</td>\n",
       "      <td>7.982364</td>\n",
       "      <td>-0.007000</td>\n",
       "      <td>86</td>\n",
       "      <td>99</td>\n",
       "      <td>91.583333</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>8.796818</td>\n",
       "      <td>9.044091</td>\n",
       "      <td>-0.247273</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>9.200455</td>\n",
       "      <td>8.885273</td>\n",
       "      <td>0.315182</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>8.733818</td>\n",
       "      <td>8.599727</td>\n",
       "      <td>0.134091</td>\n",
       "      <td>118</td>\n",
       "      <td>124</td>\n",
       "      <td>121.600000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Texas</td>\n",
       "      <td>10.604091</td>\n",
       "      <td>9.296364</td>\n",
       "      <td>1.307727</td>\n",
       "      <td>107</td>\n",
       "      <td>111</td>\n",
       "      <td>109.400000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Utah</td>\n",
       "      <td>11.951727</td>\n",
       "      <td>10.700455</td>\n",
       "      <td>1.251273</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>7.840818</td>\n",
       "      <td>8.295818</td>\n",
       "      <td>-0.455000</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>9.335909</td>\n",
       "      <td>9.071182</td>\n",
       "      <td>0.264727</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>144.714286</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Washington</td>\n",
       "      <td>10.364455</td>\n",
       "      <td>9.889636</td>\n",
       "      <td>0.474818</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>7.727000</td>\n",
       "      <td>7.820818</td>\n",
       "      <td>-0.093818</td>\n",
       "      <td>171</td>\n",
       "      <td>176</td>\n",
       "      <td>174.333333</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   State  Rate establishments born  \\\n",
       "0                Alabama                  8.094364   \n",
       "1                Arizona                 10.948182   \n",
       "2               Arkansas                  8.583182   \n",
       "3             California                 10.854000   \n",
       "4               Colorado                 11.497455   \n",
       "5            Connecticut                  8.314545   \n",
       "6               Delaware                  9.752455   \n",
       "7   District of Columbia                  9.878000   \n",
       "8                Florida                 12.417545   \n",
       "9                Georgia                 10.499727   \n",
       "10                Hawaii                  8.315182   \n",
       "11                 Idaho                 10.589909   \n",
       "12              Illinois                  9.197909   \n",
       "13               Indiana                  8.036636   \n",
       "14                  Iowa                  7.462727   \n",
       "15                Kansas                  8.135545   \n",
       "16              Kentucky                  8.276636   \n",
       "17             Louisiana                  8.481545   \n",
       "18              Maryland                  9.151000   \n",
       "19         Massachusetts                  8.739909   \n",
       "20              Michigan                  8.451545   \n",
       "21             Minnesota                  8.604455   \n",
       "22           Mississippi                  7.921000   \n",
       "23              Missouri                  9.956273   \n",
       "24              Nebraska                  8.109364   \n",
       "25                Nevada                 12.341364   \n",
       "26         New Hampshire                  8.321455   \n",
       "27            New Jersey                  9.776727   \n",
       "28            New Mexico                  8.791182   \n",
       "29              New York                 10.345636   \n",
       "30        North Carolina                  9.536364   \n",
       "31          North Dakota                  9.134545   \n",
       "32                  Ohio                  7.756455   \n",
       "33              Oklahoma                  8.944455   \n",
       "34                Oregon                  9.888273   \n",
       "35          Pennsylvania                  7.975364   \n",
       "36          Rhode Island                  8.796818   \n",
       "37        South Carolina                  9.200455   \n",
       "38             Tennessee                  8.733818   \n",
       "39                 Texas                 10.604091   \n",
       "40                  Utah                 11.951727   \n",
       "41               Vermont                  7.840818   \n",
       "42              Virginia                  9.335909   \n",
       "43            Washington                 10.364455   \n",
       "44             Wisconsin                  7.727000   \n",
       "\n",
       "    Rate establishments exited  Rate born - exited  Min rank  Max rank  \\\n",
       "0                     8.360364           -0.266000       197       202   \n",
       "1                    10.689182            0.259000       202       210   \n",
       "2                     8.523091            0.060091       210       210   \n",
       "3                    10.185727            0.668273        56        79   \n",
       "4                    10.652545            0.844909       188       188   \n",
       "5                     8.707455           -0.392909        34        36   \n",
       "6                     9.424727            0.327727       188       188   \n",
       "7                     8.755091            1.122909       135       135   \n",
       "8                    11.378727            1.038818       164       171   \n",
       "9                    10.142000            0.357727       133       135   \n",
       "10                    8.340455           -0.025273       214       214   \n",
       "11                    9.993909            0.596000       214       214   \n",
       "12                    9.300545           -0.102636        23        32   \n",
       "13                    8.122545           -0.085909       111       118   \n",
       "14                    7.417000            0.045727       194       194   \n",
       "15                    8.350273           -0.214727       202       202   \n",
       "16                    8.314818           -0.038182       210       210   \n",
       "17                    8.331727            0.149818       164       164   \n",
       "18                    9.095273            0.055727        99        99   \n",
       "19                    8.409545            0.330364         8        20   \n",
       "20                    8.726364           -0.274818       146       152   \n",
       "21                    8.468364            0.136091       183       183   \n",
       "22                    8.195000           -0.274000       214       214   \n",
       "23                    9.533545            0.422727       124       133   \n",
       "24                    7.676727            0.432636       202       202   \n",
       "25                   11.432364            0.909000       220       220   \n",
       "26                    8.381000           -0.059545       103       103   \n",
       "27                    9.990909           -0.214182         1         7   \n",
       "28                    9.232091           -0.440909       220       220   \n",
       "29                    9.862545            0.483091        37        56   \n",
       "30                    9.160818            0.375545        79        86   \n",
       "31                    7.910000            1.224545       220       220   \n",
       "32                    8.197818           -0.441364       152       164   \n",
       "33                    8.746364            0.198091       194       197   \n",
       "34                    9.348818            0.539455       202       202   \n",
       "35                    7.982364           -0.007000        86        99   \n",
       "36                    9.044091           -0.247273       103       103   \n",
       "37                    8.885273            0.315182       176       176   \n",
       "38                    8.599727            0.134091       118       124   \n",
       "39                    9.296364            1.307727       107       111   \n",
       "40                   10.700455            1.251273       183       183   \n",
       "41                    8.295818           -0.455000       197       197   \n",
       "42                    9.071182            0.264727       143       146   \n",
       "43                    9.889636            0.474818       176       176   \n",
       "44                    7.820818           -0.093818       171       176   \n",
       "\n",
       "    Average rank State with top universities  \n",
       "0     199.500000                          No  \n",
       "1     206.000000                          No  \n",
       "2     210.000000                          No  \n",
       "3      66.909091                         Yes  \n",
       "4     188.000000                          No  \n",
       "5      34.666667                         Yes  \n",
       "6     188.000000                          No  \n",
       "7     135.000000                          No  \n",
       "8     169.428571                          No  \n",
       "9     134.500000                          No  \n",
       "10    214.000000                          No  \n",
       "11    214.000000                          No  \n",
       "12     27.363636                         Yes  \n",
       "13    116.600000                          No  \n",
       "14    194.000000                          No  \n",
       "15    202.000000                          No  \n",
       "16    210.000000                          No  \n",
       "17    164.000000                          No  \n",
       "18     99.000000                         Yes  \n",
       "19     14.266667                         Yes  \n",
       "20    150.000000                          No  \n",
       "21    183.000000                          No  \n",
       "22    214.000000                          No  \n",
       "23    128.142857                          No  \n",
       "24    202.000000                          No  \n",
       "25    220.000000                          No  \n",
       "26    103.000000                         Yes  \n",
       "27      3.714286                         Yes  \n",
       "28    220.000000                          No  \n",
       "29     45.380952                         Yes  \n",
       "30     82.250000                         Yes  \n",
       "31    220.000000                          No  \n",
       "32    157.222222                          No  \n",
       "33    196.000000                          No  \n",
       "34    202.000000                          No  \n",
       "35     91.583333                         Yes  \n",
       "36    103.000000                          No  \n",
       "37    176.000000                          No  \n",
       "38    121.600000                          No  \n",
       "39    109.400000                          No  \n",
       "40    183.000000                          No  \n",
       "41    197.000000                          No  \n",
       "42    144.714286                          No  \n",
       "43    176.000000                          No  \n",
       "44    174.333333                          No  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extra.to_csv('datasets/extra_datasets_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
